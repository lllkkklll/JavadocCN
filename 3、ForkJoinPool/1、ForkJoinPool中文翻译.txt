---------------------------- ForkJoinPool类Javadoc文档翻译

一个用于运行ForkJoinTasks的ExecutorService。ForkJoinPool提供了非ForkJoinTask的客户端提交、管理和监控操作的入口。

ForkJoinPool和其他种类的ExecutorService不同, 主要在于它采用了work-stealing的优点: 池内的所有线程都尝试查找并执行提交到池和/或其他活跃任务创建的任务(如果不存在, 最终会阻塞等待工作)。当大多数任务都会衍生出其他子任务时(如执行大多数的ForkJoinTask), 以及当外部客户端提交许多小任务到池时, 这使得处理变得高效。特别是, 当构造方法中将asyncMode设置为真时, ForkJoinPool可能也适用于事件类型的且用于不会合并的任务。

静态方法commonPool()对于大多数应用是可用且合适的。公共池被任意的ForkJoinTask使用, 这个ForkJoinTask不会被显示地提交到特定的池中。使用公共池通常会减少资源使用(一段时间不使用, 池内线程会被缓慢回收, 后续使用时恢复)。

对于需要独立或自定义池的应用来说, 可以通过给定的目标并行级别来构造ForkJoinPool; 默认情况下, 等于可用处理器的数量。池试图通过动态添加、挂起或者恢复内部工作线程, 来维护足够的活跃(或可用)线程, 即使某些线程暂停等待加入其他任务。但是, 在遇到阻塞的I/O或其他非托管同步时, 不能保证进行这样的调整。嵌套的ForkJoinPool.ManagedBlocker接口可以拓展同步器适应的种类。

除了运行和生命周期的控制方法之外, 这个类还提供了状态检查方法(例如: getStealCount), 旨在帮助开发、调优和监控fork/join应用。同时, toString方法以便利的形式返回池的状态标志, 用于非正式的监控。

和其他ExecutorServices案例一样, 下面的表格统计了三个主要的任务执行方法。它们主要用于尚未参与当前池中的fork/join计算的客户端。这些方法的主要形式是接受ForkJoinTask实例, 但重载的形式也允许混合执行普通的基于Runnable-或Callable-的活动。然而, 已经在池中执行的任务通常应该使用表中列出的计算形式替代, 除非使用异步的事件风格的且通常不会组合的任务, 这种情况下方法选择上几乎没有差异。

任务执行方法大汇总
                        Call from non-fork/join clients         Call from within fork/join computations
安排异步执行                  execute(ForkJoinTask)                     ForkJoinTask.fork
等待并获取结果                invoke(ForkJoinTask)                      ForkJoinTask.invoke
安排执行并获取Future          submit(ForkJoinTask)             ForkJoinTask.fork (ForkJoinTasks are Futures)

公共池默认由默认参数构造, 但这些可以通过设置三个系统属性控制:
    1、java.util.concurrent.ForkJoinPool.common.parallelism - 并行级别, 一个正整数
    2、java.util.concurrent.ForkJoinPool.common.threadFactory - ForkJoinPool.ForkJoinWorkerThreadFactory的类名称
    3、java.util.concurrent.ForkJoinPool.common.exceptionHandler - Thread.UncaughtExceptionHandler的类名称


如果存在SecurityManager且没有指定工厂, 那么默认池使用未启用权限的提供线程的工厂。系统类加载器用于加载这些类。发布这些设置导致任何错误, 将使用默认参数。通过设置并发属性为0, 可以禁用或限制公共池中的线程使用, 且/或使用一个可能返回null的工厂。但是如此可能导致不合并任务从不执行。

实现注意项: 这个实现限制了最大的运行线程数为32767。尝试创建池比最大数更大的线程数会导致IllegalArgumentException。

这个实现拒绝提交的任务(通过抛出RejectedExecutionException), 当且仅当池被关闭或者内部资源耗尽。



##########################################  实现概览 ################################################
/*
* 实现概览
* 
* 这个类和它嵌套的类为一串worker线程提供了主要功能和控制: 来自non-FJ线程的提交进入提交队列。
* 
* Worker线程获取这些任务, 且通常会将它们分割到可以被其他worker线程偷取的子任务中。
* 优先规则优先处理来自自己队列的任务(LIFO或FIFO, 取决于模式), 然后随机的FIFO窃取其他队列中的任务。
* 
* 这个框架最初是作为使用work-stealing支持树结构并行的工具。
* 随着时间的推移, 它的可伸缩性优势导致了扩展和更改, 以更好地支持更多样化的使用上下文。
* 因为大多数内部方法和嵌套类是相互关联的, 所以这里给出了它们的主要原理和描述; 
* 单个方法和嵌套类只包含关于细节的简短注释。
* 
*
* 工作队列
* ==========
* 
* 大多数操作都会出现在work-stealing队列(在嵌套的类WorkQueue中)。双端队列有特殊的格式, 仅支持四分之三的端操作 -- push, pop和poll(又名为steal), 更进一步的限制下, push和pop只能从拥有线程调用(或者, 作为扩展, 在锁之下), 然而poll可以被其他线程调用。(如果你不熟悉它们, 你也许想阅读 Herlihy 和 Shavit 的书籍: 多处理器编程的艺术, 在继续之前, 第16章对它们进行了更详细的描述。) 
*
* work-stealing主要的队列设计类似于Chase和Lev的论文"Dynamic Circular Work-Stealing Deque", SPAA 2005(Symposium on Parallelism in Algorithms and Architectures)(http://research.sun.com/scalable/pubs/index.html)和Michael、Saraswat、Vechev、PPoPP 2009的"Idempotent work stealing"(http://portal.acm.org/citation.cfm?id=1504186)。主要的差异主要源自于GC要求, 即我们要尽快清空占用的槽, 保持尽可能小的占用空间, 即便程序生成了大量的任务。为了实现这一点, 我们将CAS仲裁是pop还是poll(窃取)从索引("底部"和"顶部")转移到槽本身。
*
* 添加任务, 然后按照经典的数组格式push(task):
*    q.array[q.top] = task; ++q.top;
*
* (准确的编码需要检查数组是否为空和检查数组大小、准确得拒绝访问并且可能的信号等待worker开始扫描 -- 查看如下。)
* 成功的pop和poll主要都需要槽的CAS操作, 将槽从非空置为空。
*
* pop操作(通常由持有线程执行):
*   if ((base != top) and
*        (the task at top slot is not null) and
*        (CAS slot to null))
*           decrement top and return task;
*
* 然后poll操作(通常由窃取线程执行):
*    if ((base != top) and
*        (the task at base slot is not null) and
*        (base has not changed) and
*        (CAS slot to null))
*           increment base and return task;
*
* 因为依赖于引用的CAS操作, 在底部或顶部我们不需要标签位。它们是简单的整数, 就像任一环绕的数组为基础的队列(查看案例ArrayDeque)。
* 对索引的更新保证top == base意味着队列为空, 
* Because we rely on CASes of references, we do not need tag bits
* on base or top.  They are simple ints as used in any circular
* array-based queue (see for example ArrayDeque).  Updates to the
* indices guarantee that top == base means the queue is empty,
* but otherwise may err on the side of possibly making the queue
* appear nonempty when a push, pop, or poll have not fully
* committed. (Method isEmpty() checks the case of a partially
* completed removal of the last element.)  Because of this, the
* poll operation, considered individually, is not wait-free. One
* thief cannot successfully continue until another in-progress
* one (or, if previously empty, a push) completes.  However, in
* the aggregate, we ensure at least probabilistic
* non-blockingness.  If an attempted steal fails, a thief always
* chooses a different random victim target to try next. So, in
* order for one thief to progress, it suffices for any
* in-progress poll or new push on any empty queue to
* complete. (This is why we normally use method pollAt and its
* variants that try once at the apparent base index, else
* consider alternative actions, rather than method poll, which
* retries.)
*
* This approach also enables support of a user mode in which
* local task processing is in FIFO, not LIFO order, simply by
* using poll rather than pop.  This can be useful in
* message-passing frameworks in which tasks are never joined.
* However neither mode considers affinities, loads, cache
* localities, etc, so rarely provide the best possible
* performance on a given machine, but portably provide good
* throughput by averaging over these factors.  Further, even if
* we did try to use such information, we do not usually have a
* basis for exploiting it.  For example, some sets of tasks
* profit from cache affinities, but others are harmed by cache
* pollution effects. Additionally, even though it requires
* scanning, long-term throughput is often best using random
* selection rather than directed selection policies, so cheap
* randomization of sufficient quality is used whenever
* applicable.  Various Marsaglia XorShifts (some with different
* shift constants) are inlined at use points.
*
* WorkQueues are also used in a similar way for tasks submitted
* to the pool. We cannot mix these tasks in the same queues used
* by workers. Instead, we randomly associate submission queues
* with submitting threads, using a form of hashing.  The
* ThreadLocalRandom probe value serves as a hash code for
* choosing existing queues, and may be randomly repositioned upon
* contention with other submitters.  In essence, submitters act
* like workers except that they are restricted to executing local
* tasks that they submitted (or in the case of CountedCompleters,
* others with the same root task).  Insertion of tasks in shared
* mode requires a lock (mainly to protect in the case of
* resizing) but we use only a simple spinlock (using field
* qlock), because submitters encountering a busy queue move on to
* try or create other queues -- they block only when creating and
* registering new queues. Additionally, "qlock" saturates to an
* unlockable value (-1) at shutdown. Unlocking still can be and
* is performed by cheaper ordered writes of "qlock" in successful
* cases, but uses CAS in unsuccessful cases.
*
* Management
* ==========
*
* The main throughput advantages of work-stealing stem from
* decentralized control -- workers mostly take tasks from
* themselves or each other, at rates that can exceed a billion
* per second.  The pool itself creates, activates (enables
* scanning for and running tasks), deactivates, blocks, and
* terminates threads, all with minimal central information.
* There are only a few properties that we can globally track or
* maintain, so we pack them into a small number of variables,
* often maintaining atomicity without blocking or locking.
* Nearly all essentially atomic control state is held in two
* volatile variables that are by far most often read (not
* written) as status and consistency checks. (Also, field
* "config" holds unchanging configuration state.)
*
* Field "ctl" contains 64 bits holding information needed to
* atomically decide to add, inactivate, enqueue (on an event
* queue), dequeue, and/or re-activate workers.  To enable this
* packing, we restrict maximum parallelism to (1<<15)-1 (which is
* far in excess of normal operating range) to allow ids, counts,
* and their negations (used for thresholding) to fit into 16bit
* subfields.
*
* Field "runState" holds lockable state bits (STARTED, STOP, etc)
* also protecting updates to the workQueues array.  When used as
* a lock, it is normally held only for a few instructions (the
* only exceptions are one-time array initialization and uncommon
* resizing), so is nearly always available after at most a brief
* spin. But to be extra-cautious, after spinning, method
* awaitRunStateLock (called only if an initial CAS fails), uses a
* wait/notify mechanics on a builtin monitor to block when
* (rarely) needed. This would be a terrible idea for a highly
* contended lock, but most pools run without the lock ever
* contending after the spin limit, so this works fine as a more
* conservative alternative. Because we don't otherwise have an
* internal Object to use as a monitor, the "stealCounter" (an
* AtomicLong) is used when available (it too must be lazily
* initialized; see externalSubmit).
*
* Usages of "runState" vs "ctl" interact in only one case:
* deciding to add a worker thread (see tryAddWorker), in which
* case the ctl CAS is performed while the lock is held.
*
* Recording WorkQueues.  WorkQueues are recorded in the
* "workQueues" array. The array is created upon first use (see
* externalSubmit) and expanded if necessary.  Updates to the
* array while recording new workers and unrecording terminated
* ones are protected from each other by the runState lock, but
* the array is otherwise concurrently readable, and accessed
* directly. We also ensure that reads of the array reference
* itself never become too stale. To simplify index-based
* operations, the array size is always a power of two, and all
* readers must tolerate null slots. Worker queues are at odd
* indices. Shared (submission) queues are at even indices, up to
* a maximum of 64 slots, to limit growth even if array needs to
* expand to add more workers. Grouping them together in this way
* simplifies and speeds up task scanning.
*
* All worker thread creation is on-demand, triggered by task
* submissions, replacement of terminated workers, and/or
* compensation for blocked workers. However, all other support
* code is set up to work with other policies.  To ensure that we
* do not hold on to worker references that would prevent GC, All
* accesses to workQueues are via indices into the workQueues
* array (which is one source of some of the messy code
* constructions here). In essence, the workQueues array serves as
* a weak reference mechanism. Thus for example the stack top
* subfield of ctl stores indices, not references.
*
* Queuing Idle Workers. Unlike HPC work-stealing frameworks, we
* cannot let workers spin indefinitely scanning for tasks when
* none can be found immediately, and we cannot start/resume
* workers unless there appear to be tasks available.  On the
* other hand, we must quickly prod them into action when new
* tasks are submitted or generated. In many usages, ramp-up time
* to activate workers is the main limiting factor in overall
* performance, which is compounded at program start-up by JIT
* compilation and allocation. So we streamline this as much as
* possible.
*
* The "ctl" field atomically maintains active and total worker
* counts as well as a queue to place waiting threads so they can
* be located for signalling. Active counts also play the role of
* quiescence indicators, so are decremented when workers believe
* that there are no more tasks to execute. The "queue" is
* actually a form of Treiber stack.  A stack is ideal for
* activating threads in most-recently used order. This improves
* performance and locality, outweighing the disadvantages of
* being prone to contention and inability to release a worker
* unless it is topmost on stack.  We park/unpark workers after
* pushing on the idle worker stack (represented by the lower
* 32bit subfield of ctl) when they cannot find work.  The top
* stack state holds the value of the "scanState" field of the
* worker: its index and status, plus a version counter that, in
* addition to the count subfields (also serving as version
* stamps) provide protection against Treiber stack ABA effects.
*
* Field scanState is used by both workers and the pool to manage
* and track whether a worker is INACTIVE (possibly blocked
* waiting for a signal), or SCANNING for tasks (when neither hold
* it is busy running tasks).  When a worker is inactivated, its
* scanState field is set, and is prevented from executing tasks,
* even though it must scan once for them to avoid queuing
* races. Note that scanState updates lag queue CAS releases so
* usage requires care. When queued, the lower 16 bits of
* scanState must hold its pool index. So we place the index there
* upon initialization (see registerWorker) and otherwise keep it
* there or restore it when necessary.
*
* Memory ordering.  See "Correct and Efficient Work-Stealing for
* Weak Memory Models" by Le, Pop, Cohen, and Nardelli, PPoPP 2013
* (http://www.di.ens.fr/~zappa/readings/ppopp13.pdf) for an
* analysis of memory ordering requirements in work-stealing
* algorithms similar to the one used here.  We usually need
* stronger than minimal ordering because we must sometimes signal
* workers, requiring Dekker-like full-fences to avoid lost
* signals.  Arranging for enough ordering without expensive
* over-fencing requires tradeoffs among the supported means of
* expressing access constraints. The most central operations,
* taking from queues and updating ctl state, require full-fence
* CAS.  Array slots are read using the emulation of volatiles
* provided by Unsafe.  Access from other threads to WorkQueue
* base, top, and array requires a volatile load of the first of
* any of these read.  We use the convention of declaring the
* "base" index volatile, and always read it before other fields.
* The owner thread must ensure ordered updates, so writes use
* ordered intrinsics unless they can piggyback on those for other
* writes.  Similar conventions and rationales hold for other
* WorkQueue fields (such as "currentSteal") that are only written
* by owners but observed by others.
*
* Creating workers. To create a worker, we pre-increment total
* count (serving as a reservation), and attempt to construct a
* ForkJoinWorkerThread via its factory. Upon construction, the
* new thread invokes registerWorker, where it constructs a
* WorkQueue and is assigned an index in the workQueues array
* (expanding the array if necessary). The thread is then
* started. Upon any exception across these steps, or null return
* from factory, deregisterWorker adjusts counts and records
* accordingly.  If a null return, the pool continues running with
* fewer than the target number workers. If exceptional, the
* exception is propagated, generally to some external caller.
* Worker index assignment avoids the bias in scanning that would
* occur if entries were sequentially packed starting at the front
* of the workQueues array. We treat the array as a simple
* power-of-two hash table, expanding as needed. The seedIndex
* increment ensures no collisions until a resize is needed or a
* worker is deregistered and replaced, and thereafter keeps
* probability of collision low. We cannot use
* ThreadLocalRandom.getProbe() for similar purposes here because
* the thread has not started yet, but do so for creating
* submission queues for existing external threads.
*
* Deactivation and waiting. Queuing encounters several intrinsic
* races; most notably that a task-producing thread can miss
* seeing (and signalling) another thread that gave up looking for
* work but has not yet entered the wait queue.  When a worker
* cannot find a task to steal, it deactivates and enqueues. Very
* often, the lack of tasks is transient due to GC or OS
* scheduling. To reduce false-alarm deactivation, scanners
* compute checksums of queue states during sweeps.  (The
* stability checks used here and elsewhere are probabilistic
* variants of snapshot techniques -- see Herlihy & Shavit.)
* Workers give up and try to deactivate only after the sum is
* stable across scans. Further, to avoid missed signals, they
* repeat this scanning process after successful enqueuing until
* again stable.  In this state, the worker cannot take/run a task
* it sees until it is released from the queue, so the worker
* itself eventually tries to release itself or any successor (see
* tryRelease).  Otherwise, upon an empty scan, a deactivated
* worker uses an adaptive local spin construction (see awaitWork)
* before blocking (via park). Note the unusual conventions about
* Thread.interrupts surrounding parking and other blocking:
* Because interrupts are used solely to alert threads to check
* termination, which is checked anyway upon blocking, we clear
* status (using Thread.interrupted) before any call to park, so
* that park does not immediately return due to status being set
* via some other unrelated call to interrupt in user code.
*
* Signalling and activation.  Workers are created or activated
* only when there appears to be at least one task they might be
* able to find and execute.  Upon push (either by a worker or an
* external submission) to a previously (possibly) empty queue,
* workers are signalled if idle, or created if fewer exist than
* the given parallelism level.  These primary signals are
* buttressed by others whenever other threads remove a task from
* a queue and notice that there are other tasks there as well.
* On most platforms, signalling (unpark) overhead time is
* noticeably long, and the time between signalling a thread and
* it actually making progress can be very noticeably long, so it
* is worth offloading these delays from critical paths as much as
* possible. Also, because inactive workers are often rescanning
* or spinning rather than blocking, we set and clear the "parker"
* field of WorkQueues to reduce unnecessary calls to unpark.
* (This requires a secondary recheck to avoid missed signals.)
*
* Trimming workers. To release resources after periods of lack of
* use, a worker starting to wait when the pool is quiescent will
* time out and terminate (see awaitWork) if the pool has remained
* quiescent for period IDLE_TIMEOUT, increasing the period as the
* number of threads decreases, eventually removing all workers.
* Also, when more than two spare threads exist, excess threads
* are immediately terminated at the next quiescent point.
* (Padding by two avoids hysteresis.)
*
* Shutdown and Termination. A call to shutdownNow invokes
* tryTerminate to atomically set a runState bit. The calling
* thread, as well as every other worker thereafter terminating,
* helps terminate others by setting their (qlock) status,
* cancelling their unprocessed tasks, and waking them up, doing
* so repeatedly until stable (but with a loop bounded by the
* number of workers).  Calls to non-abrupt shutdown() preface
* this by checking whether termination should commence. This
* relies primarily on the active count bits of "ctl" maintaining
* consensus -- tryTerminate is called from awaitWork whenever
* quiescent. However, external submitters do not take part in
* this consensus.  So, tryTerminate sweeps through queues (until
* stable) to ensure lack of in-flight submissions and workers
* about to process them before triggering the "STOP" phase of
* termination. (Note: there is an intrinsic conflict if
* helpQuiescePool is called when shutdown is enabled. Both wait
* for quiescence, but tryTerminate is biased to not trigger until
* helpQuiescePool completes.)
*
*
* Joining Tasks
* =============
*
* Any of several actions may be taken when one worker is waiting
* to join a task stolen (or always held) by another.  Because we
* are multiplexing many tasks on to a pool of workers, we can't
* just let them block (as in Thread.join).  We also cannot just
* reassign the joiner's run-time stack with another and replace
* it later, which would be a form of "continuation", that even if
* possible is not necessarily a good idea since we may need both
* an unblocked task and its continuation to progress.  Instead we
* combine two tactics:
*
*   Helping: Arranging for the joiner to execute some task that it
*      would be running if the steal had not occurred.
*
*   Compensating: Unless there are already enough live threads,
*      method tryCompensate() may create or re-activate a spare
*      thread to compensate for blocked joiners until they unblock.
*
* A third form (implemented in tryRemoveAndExec) amounts to
* helping a hypothetical compensator: If we can readily tell that
* a possible action of a compensator is to steal and execute the
* task being joined, the joining thread can do so directly,
* without the need for a compensation thread (although at the
* expense of larger run-time stacks, but the tradeoff is
* typically worthwhile).
*
* The ManagedBlocker extension API can't use helping so relies
* only on compensation in method awaitBlocker.
*
* The algorithm in helpStealer entails a form of "linear
* helping".  Each worker records (in field currentSteal) the most
* recent task it stole from some other worker (or a submission).
* It also records (in field currentJoin) the task it is currently
* actively joining. Method helpStealer uses these markers to try
* to find a worker to help (i.e., steal back a task from and
* execute it) that could hasten completion of the actively joined
* task.  Thus, the joiner executes a task that would be on its
* own local deque had the to-be-joined task not been stolen. This
* is a conservative variant of the approach described in Wagner &
* Calder "Leapfrogging: a portable technique for implementing
* efficient futures" SIGPLAN Notices, 1993
* (http://portal.acm.org/citation.cfm?id=155354). It differs in
* that: (1) We only maintain dependency links across workers upon
* steals, rather than use per-task bookkeeping.  This sometimes
* requires a linear scan of workQueues array to locate stealers,
* but often doesn't because stealers leave hints (that may become
* stale/wrong) of where to locate them.  It is only a hint
* because a worker might have had multiple steals and the hint
* records only one of them (usually the most current).  Hinting
* isolates cost to when it is needed, rather than adding to
* per-task overhead.  (2) It is "shallow", ignoring nesting and
* potentially cyclic mutual steals.  (3) It is intentionally
* racy: field currentJoin is updated only while actively joining,
* which means that we miss links in the chain during long-lived
* tasks, GC stalls etc (which is OK since blocking in such cases
* is usually a good idea).  (4) We bound the number of attempts
* to find work using checksums and fall back to suspending the
* worker and if necessary replacing it with another.
*
* Helping actions for CountedCompleters do not require tracking
* currentJoins: Method helpComplete takes and executes any task
* with the same root as the task being waited on (preferring
* local pops to non-local polls). However, this still entails
* some traversal of completer chains, so is less efficient than
* using CountedCompleters without explicit joins.
*
* Compensation does not aim to keep exactly the target
* parallelism number of unblocked threads running at any given
* time. Some previous versions of this class employed immediate
* compensations for any blocked join. However, in practice, the
* vast majority of blockages are transient byproducts of GC and
* other JVM or OS activities that are made worse by replacement.
* Currently, compensation is attempted only after validating that
* all purportedly active threads are processing tasks by checking
* field WorkQueue.scanState, which eliminates most false
* positives.  Also, compensation is bypassed (tolerating fewer
* threads) in the most common case in which it is rarely
* beneficial: when a worker with an empty queue (thus no
* continuation tasks) blocks on a join and there still remain
* enough threads to ensure liveness.
*
* The compensation mechanism may be bounded.  Bounds for the
* commonPool (see commonMaxSpares) better enable JVMs to cope
* with programming errors and abuse before running out of
* resources to do so. In other cases, users may supply factories
* that limit thread construction. The effects of bounding in this
* pool (like all others) is imprecise.  Total worker counts are
* decremented when threads deregister, not when they exit and
* resources are reclaimed by the JVM and OS. So the number of
* simultaneously live threads may transiently exceed bounds.
*
* Common Pool
* ===========
*
* The static common pool always exists after static
* initialization.  Since it (or any other created pool) need
* never be used, we minimize initial construction overhead and
* footprint to the setup of about a dozen fields, with no nested
* allocation. Most bootstrapping occurs within method
* externalSubmit during the first submission to the pool.
*
* When external threads submit to the common pool, they can
* perform subtask processing (see externalHelpComplete and
* related methods) upon joins.  This caller-helps policy makes it
* sensible to set common pool parallelism level to one (or more)
* less than the total number of available cores, or even zero for
* pure caller-runs.  We do not need to record whether external
* submissions are to the common pool -- if not, external help
* methods return quickly. These submitters would otherwise be
* blocked waiting for completion, so the extra effort (with
* liberally sprinkled task status checks) in inapplicable cases
* amounts to an odd form of limited spin-wait before blocking in
* ForkJoinTask.join.
*
* As a more appropriate default in managed environments, unless
* overridden by system properties, we use workers of subclass
* InnocuousForkJoinWorkerThread when there is a SecurityManager
* present. These workers have no permissions set, do not belong
* to any user-defined ThreadGroup, and erase all ThreadLocals
* after executing any top-level task (see WorkQueue.runTask).
* The associated mechanics (mainly in ForkJoinWorkerThread) may
* be JVM-dependent and must access particular Thread class fields
* to achieve this effect.
*
* Style notes
* ===========
*
* Memory ordering relies mainly on Unsafe intrinsics that carry
* the further responsibility of explicitly performing null- and
* bounds- checks otherwise carried out implicitly by JVMs.  This
* can be awkward and ugly, but also reflects the need to control
* outcomes across the unusual cases that arise in very racy code
* with very few invariants. So these explicit checks would exist
* in some form anyway.  All fields are read into locals before
* use, and null-checked if they are references.  This is usually
* done in a "C"-like style of listing declarations at the heads
* of methods or blocks, and using inline assignments on first
* encounter.  Array bounds-checks are usually performed by
* masking with array.length-1, which relies on the invariant that
* these arrays are created with positive lengths, which is itself
* paranoically checked. Nearly all explicit checks lead to
* bypass/return, not exception throws, because they may
* legitimately arise due to cancellation/revocation during
* shutdown.
*
* There is a lot of representation-level coupling among classes
* ForkJoinPool, ForkJoinWorkerThread, and ForkJoinTask.  The
* fields of WorkQueue maintain data structures managed by
* ForkJoinPool, so are directly accessed.  There is little point
* trying to reduce this, since any associated future changes in
* representations will need to be accompanied by algorithmic
* changes anyway. Several methods intrinsically sprawl because
* they must accumulate sets of consistent reads of fields held in
* local variables.  There are also other coding oddities
* (including several unnecessary-looking hoisted null checks)
* that help some methods perform reasonably even when interpreted
* (not compiled).
*
* The order of declarations in this file is (with a few exceptions):
* (1) Static utility functions
* (2) Nested (static) classes
* (3) Static fields
* (4) Fields, along with constants used when unpacking some of them
* (5) Internal control methods
* (6) Callbacks and other support for ForkJoinTask methods
* (7) Exported methods
* (8) Static block initializing statics in minimally dependent order
*/